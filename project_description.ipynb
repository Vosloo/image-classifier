{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Project 1 - Convolutional Neural Networks\n",
    "\n",
    "## Basic information\n",
    "\n",
    "### Rules for completing the course\n",
    "\n",
    "- The grade for this project is $50\\%$ of the final grade for the Deep Learning course\n",
    "- In order to pass the course, it is necessary to pass ($\\geq 50\\%$) each of the 2 projects\n",
    "\n",
    "### Purpose of the project\n",
    "\n",
    "- Getting acquainted with one of the most popular frameworks used in Deep Learning - TensorFlow\n",
    "- Practical exercises related to Deep Learning (in particular Convolutional Neural Networks)\n",
    "- Empirical confirmation of the influence of individual elements of the algorithm and data on the results (based on experimental comparative analyzes)\n",
    "- Improving programming skills\n",
    "\n",
    "### Project prerequisites\n",
    "\n",
    "- Student has the skills allowing to independently implement the code in Python, is able to use Jupyter Notebook\n",
    "- Student is able to search for information on the documentation of Python packages and use them in practice\n",
    "- Student has knowledge of the basics of Deep Learning, in particular about the Convolutional Neural Networks (knowledge obtained during laboratories; materials available at https://www.cs.put.poznan.pl/gmiebs/students/dl/)\n",
    "\n",
    "### Implementation of the project\n",
    "\n",
    "- Individually or in groups of two\n",
    "\n",
    "### Parts of the project\n",
    "\n",
    "Part 1 - Implementation of an image classifier ($35\\%$)\n",
    "\n",
    "Part 2 - Experimental comparative analyses ($25\\%$ for each subtask):\n",
    " - Task's implementation ($15\\%$)\n",
    " - Task's description ($10\\%$)\n",
    " \n",
    "Maximum rating for the project - $100\\%$.\n",
    " \n",
    "**Note:** If one of the task parts (implementation or description) is missing, the latter does not count. The percentages are only used to show how which part contributes to the assessment.\n",
    " \n",
    "**Note:** Without correctly completing the Part 1, it will likely be difficult (or even impossible) to complete tasks in the Part 2.\n",
    "\n",
    "### Technical requirements\n",
    "\n",
    "- Implementation of the solution in Python, using the TensorFlow package with the Keras API. You can use TensorFlow with GPU support, but it's not mandatory.\n",
    "- Preferred form - Notebook(s). It is also possible to implement in regular Python files (.py).\n",
    "- Suggested support packages (e.g. for displaying plots, tables, dataset preparation etc.) - NumPy, matplotlib, pandas, scikit-learn (others can be used)\n",
    "\n",
    "### Deadline\n",
    "\n",
    "- Midnight between 2021-11-14 / 2021-11-15\n",
    "- In case of delay, each subsequent week started will lower the grade by 10%\n",
    "\n",
    "### Solution content\n",
    "\n",
    "Please:\n",
    "\n",
    "- Provide the source code performing tasks from the Parts 1 and 2. As long as the Notebook is not too big (over a few MB) - please **don't** reset the Notebook(s) output. If I have problems testing the code, I will be able to check your solutions at least by looking at the output\n",
    "- Attach also a list of packages with the versions you used when implementing the project (*pip freeze > requirements.txt*)\n",
    "- If you are not using Markdowns in Notebook(s) or comments in case of regular Python files, please describe in the email where I should look for the code for specific parts of the tasks.\n",
    "- If you decide to prepare a separate report (preffered PDF file), attach it as well.\n",
    "- If the file structure is different than that proposed in the content of the tasks, provide information about the file structure - i.e. where should I put the dataset in order to run your code\n",
    "- Provide any graphs, tables, files, resources etc. on the basis of which conclusions about the tasks were made, and are not visible in the Markdowns or output of the Notebook. Please describe in the report or in the e-mail what the attached materials relate to.\n",
    "- Add files with model and parameters from Part 1, task 6) (as long as they do not occupy more than a few MB)\n",
    "\n",
    "Please don't:\n",
    "\n",
    "- Send the dataset - I'd rather avoid downloading mega- or gigabytes of data from each of you :)\n",
    "\n",
    "### Delivery method\n",
    "\n",
    "- Send the above-mentioned content (ZIP file preferred) to michal.wojcik@doctorate.put.poznan.pl\n",
    "- Title: **[DL] Project 1 - [Student1_First_Name] [Student1_Surname] [Student1_ID], [Student2_First_Name] [Student2_Surname] [Student2_ID]** (Erazmus students - omit the ID)\n",
    "- Title example: **[DL] Project 1 - Anna Nowak 123456, John Doe 789012**\n",
    "\n",
    "### Project modifications\n",
    "\n",
    "This document is a project proposal and describes the preferred way to complete and pass the first part of the Deep Learning Labs.\n",
    "\n",
    "If you believe that introducing minor (e.g. changing the dataset, new task definition) or major modifications to this project or changing the approach (e.g. implementing a different project that solves another practical problem - mainly for those who are more experienced in Deep Learning) will be more interesting and will allow you to better understand and learn about previously unknown issues related to the basics of Deep Learning (in particular Convolutional Neural Networks), please do not hesitate to present your proposal to modify the project definition. \n",
    "\n",
    "In order for the proposed changes to be accepted, the concept must be presented individually (by e-mail or during the class), then described in detail and approved via e-mail by the teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Implementation of an image classifier\n",
    "\n",
    "The goal of the Part 1 is to implement an image classifier from scratch. The task consists in loading the data (images & labels), preprocessing, preparing the datasets for learning process, preparing the model, conducting the learning and evaluating the model on the test set. A more detailed scheme is presented below, broken down into subtasks. \n",
    "\n",
    "### Data\n",
    "Proposed datasets:\n",
    "\n",
    "- Caltech-101 (http://www.vision.caltech.edu/Image_Datasets/Caltech101/) (131 MB)\n",
    "- Caltech-256 (http://www.vision.caltech.edu/Image_Datasets/Caltech256/) (1.2 GB)\n",
    "\n",
    "Both of the above collections consist of multiple JPG images containing an object from one of the many classes. For each class of objects, there are from several dozen to several hundred examples of images.\n",
    "\n",
    "If you decide you want to use a different dataset, here are some requirements:\n",
    "- The size of the images should not be smaller than 100x100 pixels\n",
    "- Images should be in color (RGB/RGBA), not grayscale\n",
    "- Images should be in such a format that they can be easily loaded into numpy.array of pixels (png, jpg)\n",
    "- The collection should contain at least 20 classes with at least 80 examples for each class\n",
    "- Images must be labeled - that is, assigned exactly to 1 of N decision classes\n",
    "- The classification problem cannot be trivial (e.g. it is possible to assign photos to classes by calculating the average pixel color)\n",
    "\n",
    "As mentioned above, the proposed change of the dataset must be accepted.\n",
    "\n",
    "### Before you start\n",
    "\n",
    "- Start by reading all the steps and see also the Part 2 - this will make it easier for you to plan the entire flow and how to organize your code\n",
    "- Implement tasks as a functions - it's always a good idea if your code is reusable and parameterizable, especially in that case, because some part of the source code will be useful in the Part 2\n",
    "- Use Markdowns - they help organize the code and allow you to describe additional information. Each time the task includes a question or request to describe your conclusions, comments, observations, etc. your answer should be put in Markdowns (preferred form) or in the report file that you send after completing all the tasks\n",
    "- **Step verification** should also be implemented - it should be visible as the output of the cell(s) execution after implementing and executing the code that carries out the tasks specified in a given step\n",
    "- The hints, order of subtasks, functions mentioned in the description below are only a suggestion - you can present your own approach and improvements, but in such a way that it is possible to observe the changes taking place between subsequent steps Please, don't use *magic*, one-line functions that do all the steps at once - the aim of the project is also to familiarize you with the features offered by the packages (mainly TensorFlow with Keras API). If you have made any exceptions from the presented approach (e.g. you loaded a dataset with the tensorflow_datasets package), please mention and describe this fact in the step 7)\n",
    "\n",
    "### Steps to follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Download the data and load it in the Notebook (5%)\n",
    "\n",
    "**Note:** The *traditional* dataset loading sequence is described below. You can also use tensorflow_datasets package for this purpose.\n",
    "\n",
    "- Download the archive with the dataset\n",
    "- Create directory \"data\" and extract downloaded files to it\n",
    "- Implement loading images and labels\n",
    "- Each image should be represented in the form of numpy.array (shape: (height, width, channels))\n",
    "- Load all interesting images and labels into two lists\n",
    "\n",
    "**Note:** Watch out for file extensions!\n",
    "\n",
    "**Step verification:**\n",
    "\n",
    "Display one of the loaded images, print out the shape of the image, check if the label is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Standardize the images (5%)\n",
    "\n",
    "**Note:** Some of these operations can be performed while files are being loaded\n",
    "\n",
    "Unify the images:\n",
    "\n",
    "- Number and sequence of channels (RGB) if needed\n",
    "- Images shape (e.g. $32 \\times 32 \\times 3$) - including channel convention (channels_last suggested)\n",
    "- Standardization of pixel values ($\\frac{x - \\mu}{\\sigma}$) - calculate $\\mu$ and $\\sigma$ for the whole dataset, separately for each channel\n",
    "\n",
    "**Step verification:**\n",
    "\n",
    "Check what the image selected in step 1) now looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Divide the collection into Train and Test set (5%)\n",
    "\n",
    "\n",
    "- Reduce the number of classes - filter the collection and leave images from ~15-25 classes, select those classes that have the largest number of examples. Make sure your collection is balanced (roughly the same number of samples for each class). You can do this by discarding classes that cause imbalance, or you can reduce the number of samples in larger classes.\n",
    "- Randomly split the set into train (70%) and test (30%) set - X_train, X_test (images) and y_train, y_test (labels). (hint: check *sklearn.model_selection.train_test_split*)\n",
    "- Make sure that the proportions of each class in both sets are more or less the same as in the whole set (hint: *stratify* parameter in *train_test_split*)\n",
    "- Change labels in y vectors to one-hot encoding\n",
    "- Ensure image and label collections are in the form of numpy.array\n",
    "\n",
    "**Step verification:**\n",
    "\n",
    "- Check the shape of the X_train, X_test (images) and the y_train, y_test (labels)\n",
    "- Check on the example image if the label is in the correct form\n",
    "- Check how many samples from each class are in particular subsets (train and test) and if the proportions are kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Define the model (5%)**\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "- Activation functions - *ReLU*\n",
    "- At least 3 *Convolutional blocks* - (Conv2D, Activation, BatchNormalization, Dropout, MaxPooling2D); Conv2D - *kernel=(3,3)*, *padding='same'*; MaxPooling2D - *pool_size=(2,2)*\n",
    "- Flatten layer\n",
    "- At least 2 layers Fully-Connected (Dense)\n",
    "- Output layer - Dense with number_of_classes outputs (remember to use softmax)\n",
    "\n",
    "**Note:** You can add *Activation* as a separate layer or as *activation='relu'* parameter in Conv2D\n",
    "\n",
    "**Step verification:**\n",
    "\n",
    "Compile the model with *'adam'* optimizer, the Categorical Crossentropy as the loss function, and measure the accuracy value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Train the model (5%)**\n",
    "\n",
    "Suggested hyperparameters:\n",
    "\n",
    "- Batch_size = 32\n",
    "- Epochs = 250 (first, just check if everything works on several epochs to save time)\n",
    "- Monitor the value of measures for the test set\n",
    "- Add [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) - stop training when there is no improvement in accuracy for the test set within 5 consecutive epochs (you can first test the behavior for patience = 1)\n",
    "\n",
    "**Step verification:**\n",
    "\n",
    "After completing the learning process, show:\n",
    "\n",
    "- Learning curves for loss_function and accuracy, changing over the epochs on train and test set\n",
    "- Confusion matrix and calculate precision and recall for each class for test set\n",
    "- Show few images from the test set, display the probabilities of assigning them to each class (at least two examples: one image example that was classified correctly and one which was classified incorrectly)\n",
    "\n",
    "**Note:** Functions to display learning curves and confusion matrices will be useful in the Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Save the model to disk (5%)**\n",
    "\n",
    "- Prepare 2 functions - for saving the model and for loading the model\n",
    "- Model structure should be saved as JSON file, model parameters in HDF5 file\n",
    "\n",
    "**Step verification:**\n",
    "\n",
    "Checking the operation of both functions by:\n",
    "\n",
    "- Save the model\n",
    "- Load the model\n",
    "- Make predictions on the loaded model for the test set\n",
    "- Display the confusion matrix for the test set and compare it with the matrix obtained in the previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Summary of the Part 1 - describe your observations (5%)**\n",
    "\n",
    "Describe your observations on the tasks performed. Supporting questions:\n",
    "\n",
    "- What kind of modifications have you made? Why? (*Describe, if you made any*)\n",
    "- What results have you achieved?\n",
    "- Is the underfitting or overfitting of the model visible? If so, what are your suggestions for solving this problem?\n",
    "- Which class(es) the model had a problem with? Which it did best with? Which pair of classes were most often confused with each other? Can you guess why?\n",
    "- What improvement opportunities do you see?\n",
    "\n",
    "**Step verification:**\n",
    "\n",
    "Use Markdowns to describe your conclusions or put them into the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Experimental comparative analysis\n",
    "\n",
    "The purpose of the Part 2 is to examine the dependence of the quality of the resulting model on factors such as hyperparameters, model structure, number of training data, number of decision classes, etc. Below is a suggestion of simple tasks - most of them consist of a simple experiment to compare several models that differ in some detail.\n",
    "\n",
    "### Before you start\n",
    "\n",
    "- You don't have to complete all of the tasks, you can choose the issues that seem interesting to you. You can also suggest your own ideas for the experiment definition (in that case - let me know). As mentioned above, each task is worth $25\\%$ of the project grade (the value of the last task is doubled). Assuming you've completed the Part 1, as you may have already calculated:\n",
    "  - You should complete at least 1 task *almost* correctly to pass ($35\\% + 25\\% = 60\\% \\geq 50\\%$)\n",
    "  - You should complete at least 3 tasks *almost* correctly to get the maximum score for a Project ($35\\% + 3 \\cdot 25\\% = 110\\% \\rightarrow 100\\%$)\n",
    "  \n",
    "  You can complete more than 3 tasks, because it may save you from a possible loss of points and will allow you to obtain a satisfactory result.\n",
    "- It is possible to combine tasks into one larger experiment - most of the tasks consist of comparing models that differ in one aspect. Instead of conducting individual experiments, you can perform [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) and modify a set of several parameters. However, it is important to remember that the conclusions about a given experiment should take into account observations about each of the subtasks.\n",
    "- If you perform several experiments one after another, take into account the conclusions from the previous ones. For example, if in the first experiment you are examining the differences between the quality of individual models, and in the next experiment you are examining the effect of the size of the train set, then use the better model from the first experiment.\n",
    "- If you have not already done so, modify the functions in the Part 1 so that they can be used for the following subtasks. You can also rewrite the code and adapt it to the requirements of the tasks.\n",
    "- Pay attention to the last task - it is more demanding and requires you to find additional information, therefore it is worth 50%.\n",
    "\n",
    "### Collecting experimental data\n",
    "\n",
    "In addition to implementing the code and running experiments to carry out a given task, conclusions should be formulated about the results obtained. In that case, it may be helpful to collect the following information about the models compared:\n",
    "\n",
    "- The number of network parameters (as a measure of the memory complexity of the model)\n",
    "- Training time (as a measure of the temporal complexity of the model)\n",
    "- Values of the obtained quality measures (loss function, accuracy - maybe some other measures?) For train and set test in the resulting model\n",
    "- Values of loss_function and measures in successive epochs (learning curves - can be useful in the context of discussing overfitting, optimization speed and model stability while learning)\n",
    "- Confusion matrix (mainly in the test set, but maybe the train set matrix will help to better understand the learning process)\n",
    "- Precision and recall (maybe some other measures?) for decision classes (assessment of classification difficulty for individual classes; comparison of the precision-recall curve between classes and between models)\n",
    "- Display some examples that were correctly and incorrectly classified by one or more models (example-based explanation)\n",
    "\n",
    "In general, there are many possibilities to visualize data, to compare models with each other and draw conclusions from it.\n",
    "\n",
    "### Task description\n",
    "\n",
    "Each of the following tasks assumes the implementation of an appropriate code that will carry out a given experiment - and therefore it will probably modify the dataset or model (its structure or hyperparameters). After implementing the experiment, write:\n",
    "- Which task did you choose?\n",
    "- What are the differences between the models or data sets you tested?\n",
    "\n",
    "After obtaining the results for the compared models or datasets, describe your observations and conclusions about the experiment. Observations may in particular concern:\n",
    "- The overall predictive ability of the model - which model is better, which model is worse, maybe one is better with class X and one with class Y, etc.\n",
    "- The impact of the changes made on overfitting and underfitting\n",
    "- Comparison of the duration of learning, the complexity of the model\n",
    "- Assessment of whether it was profitable to make such a modification\n",
    "- Learning curve shape - in which model the optimization was faster, whether the loss function and accuracy on the set test decreased steadily, were there any deviations, etc.\n",
    "- Sample images where one model is better than the other - try to answer *why?*\n",
    "- If something is not working well, write about your assumptions, why it failed and what could be changed. If you want, try the changes you suggest and also describe whether it has brought the desired effect.\n",
    "\n",
    "In general, it is worth describing everything that you find interesting in the context of the specific task. The description of the conclusions should be included in Markdown(s) (or in the report if you want to prepare one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "#### 1) The impact of the size of the training set on the results - choose one option:\n",
    "- Compare the results achieved by the same model, e.g. for 15-20 classes that have ~ 80 samples and 15-20 classes that have ~ 30 samples\n",
    "- You can also use the same classes and reduce the number of samples in the training set (e.g. 50, 40, 30, 20 samples in the training set)\n",
    "- Another idea - regulate the proportion of the division of the set into Train and Test set (e.g. 80-20, 60-40, 40-60, 20-80)\n",
    "\n",
    "#### 2) The impact of the number of decision classes on the results (e.g. 10, 25, 50, all of the available classes)\n",
    "\n",
    "#### 3) Compare the models without and with Dropout with different rates (e.g. 0.1, 0.2, 0.5)\n",
    "\n",
    "#### 4) Compare the models without and with regularization (L1, L2, L1 + L2)\n",
    "\n",
    "#### 5) Compare the models without and with batch normalization (before or after the activation function)\n",
    "\n",
    "#### 6) Compare the models for different preprocessing approaches (perform operations separately per channel):\n",
    "- Raw data - $X$\n",
    "- Subtracting the mean ($X - \\mu$)\n",
    "- Normalization ($\\frac{X - min}{max - min}$)\n",
    "- Standardization ($\\frac{X - \\mu}{\\sigma}$)\n",
    "\n",
    "#### 7) Compare the models with different activation functions (ReLU, tanh, sigmoid - you can use others as well)\n",
    "\n",
    "#### 8) Compare the models with different Pooling layers (MaxPooling, AveragePooling; you can also check GlobalMaxPooling and GlobalAveragePooling after the last convolution layer)\n",
    "\n",
    "#### 9) Compare the models with different number of Convolutional blocks\n",
    "\n",
    "#### 10) Compare the models with different number of Neurons in each Convolutional block\n",
    "\n",
    "#### 11) Training with different batch sizes (e.g. 1, 16, 32, number_of_samples)\n",
    "\n",
    "#### 12) Training for unbalanced datasets  (Task double scored, worth 50%)\n",
    "- Take one class with a lot of samples (e.g. in Caltech101 - airplanes with 800 samples) and 1 or several classes with significantly fewer samples (e.g. 1 class with ~80 samples or 5 classes with ~30 samples per class)\n",
    "- Split the dataset proportionally in the Train and Test set\n",
    "- Train the default model - what was your accuracy, precision, recall?\n",
    "- Check what values would you get if you made a simple decision rule model which always classifies samples into the most numerous class? Is your model clearly better than it or has it achieved quite similar results?\n",
    "- Find information about traning on the imbalanced dataset and how to deal with that problem e.g. change loss function, change quality measures, check how you can modify dataset\n",
    "- Apply the changes and check if you managed to get a better model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
